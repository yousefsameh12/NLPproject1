import re
from pathlib import Path

def load_texts(local_alice: str = '../data/48LawsOfPower.txt',
               local_glass: str = '../data/LawsOfHumanNature.txt'):
    """Load 48 Laws and Laws of Human Nature texts from disk.

    Parameters
    ----------
    local_alice : str
        Path to 48 Laws text file. Defaults to '../data/48LawsOfPower.txt'.
    local_glass : str
        Path to Laws of Human Nature text file. Defaults to '../data/LawsOfHumanNature.txt'.

    Returns
    -------
    tuple[str, str]
        (laws_text, human_text).

    Raises
    ------
    FileNotFoundError
        If either file is missing.

    Extra Notes
    -----------
    - Using UTF-8 with errors='ignore' avoids codec exceptions on
      older Project Gutenberg dumps or inconsistent encodings.
    """
    p1, p2 = Path(local_alice), Path(local_glass)

    # Fail fast with a clear message if a file is missing
    if not p1.exists():
        raise FileNotFoundError(
            f"Missing file: {p1}\n"
            "→ Please place '48LawsOfPower.txt' at this path or update load_texts(...)."
        )
    if not p2.exists():
        raise FileNotFoundError(
            f"Missing file: {p2}\n"
            "→ Please place 'LawsOfHumanNature.txt' at this path or update load_texts(...)."
        )

    # Read the files (UTF-8; ignore undecodable bytes to stay robust)
    wonderland   = p1.read_text(encoding='utf-8', errors='ignore')
    lookingglass = p2.read_text(encoding='utf-8', errors='ignore')
    return wonderland, lookingglass

def normalize(text: str) -> str:
    """Normalize a Gutenberg-like text for tokenization.

    Steps
    -----
    1) Heuristically strip Project Gutenberg headers/footers if present
       (looks for *** START ... *** END markers).
    2) Normalize newlines to '\n'.

    Parameters
    ----------
    text : str
        Raw text as loaded from disk (can be empty).

    Returns
    -------
    str
        Cleaned text suitable for tokenization and counting.
    """
    if not text:
        return ''
    # Clip to the main body if markers are present.
    start = text.find('*** START')
    end   = text.find('*** END')
    if start != -1 and end != -1 and end > start:
        text = text[start:end]
    # Normalize Windows line endings.
    return text.replace('\r\n', '\n')

# Load raw texts with error handling
try:
    wonderland_raw, lookingglass_raw = load_texts()
    print("Texts loaded successfully.")
except FileNotFoundError as e:
    print(f"File error: {e}")
    print("Using fallback sample texts for demo.")
    wonderland_raw = "Law 1: Never Outshine the Master. Always make those above you feel comfortably superior. In your desire to please or impress them, do not go too far in displaying your talents or you might accomplish the opposite—inspire fear and insecurity. Make your masters appear more brilliant than they are and you will attain the heights of power."
    lookingglass_raw = "The Laws of Human Nature reveal the hidden forces that drive human behavior. People are irrational, driven by emotions and self-interest. Understanding these laws allows you to navigate social dynamics more effectively. For example, the law of irrationality shows how people act against their own interests due to biases and desires."

# Normalize for tokenization
wonderland   = normalize(wonderland_raw)
lookingglass = normalize(lookingglass_raw)

print(f"48 Laws chars: {len(wonderland):,} | Human Nature chars: {len(lookingglass):,}")
WORD_RE = re.compile(r"[A-Za-z']+")  # keep apostrophes in words (e.g., don't -> don't)

def words(text: str):
    """Simple word tokenizer (lowercased, ASCII letters + apostrophes).

    Pros
    ----
    - Very fast and dependency-free.
    - Good enough for frequency/keyness demonstrations.

    Cons
    ----
    - No punctuation words, no sentence boundaries, no POS tags.
    - May treat possessives inconsistently across sources.

    Returns
    -------
    list[str]
        Lowercased word words.
    """
    return WORD_RE.findall(text.lower())

def sentences(text: str):
    """Naive sentence splitter using punctuation boundaries.

    Uses a regex to split on '.', '!', '?' followed by whitespace.
    Because this is heuristic, treat results as approximate.

    Returns
    -------
    list[str]
        Sentence-like strings.
    """
    return [s.strip() for s in re.split(r'(?<=[.!?])\s+', text) if s.strip()]

wonderland_words = words(wonderland)
lookingglass_words = words(lookingglass)

wonderland_sentences = sentences(wonderland)
lookingglass_sentences = sentences(lookingglass)

print(f"48 Laws words: {len(wonderland_words):,} | Human Nature words: {len(lookingglass_words):,}")
print(f"48 Laws sentences: {len(wonderland_sentences):,} | Human Nature sentences: {len(lookingglass_sentences):,}")

def adverb_rate(words):
    adverbs = [w for w in words if w.endswith('ly') and len(w)>2]
    return len(adverbs), len(words), (len(adverbs)/len(words))*100

a_adv, a_total, a_pct = adverb_rate(wonderland_words)
g_adv, g_total, g_pct = adverb_rate(lookingglass_words)
print(f"48 Laws: {a_adv}/{a_total} = {a_pct:.2f}%")
print(f"Human Nature: {g_adv}/{g_total} = {g_pct:.2f}%")
# Show some adverbs from 48 Laws
adverbs_wonderland = [w for w in wonderland_words if w.endswith('ly') and len(w)>2]
print(f"48 Laws adverbs (first 20): {adverbs_wonderland[:20]}")

# SpaCy integration with error handling
try:
    import spacy
    from spacy.cli import download

    try:
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        print("Downloading spaCy model...")
        download("en_core_web_sm")
        nlp = spacy.load("en_core_web_sm")

    def strip_gutenberg_markup(t: str) -> str:
        # remove lone underscores and italic markup
        t = re.sub(r"\b_+\b", " ", t)
        t = re.sub(r"([A-Za-z]+)", r"\1", t)
        return t

    # 1) Pre-clean, then parse
    text_clean = strip_gutenberg_markup(wonderland)  # or your variable
    doc = nlp(text_clean)

    true_adverbs = [t.text for t in doc if t.pos_ == "ADV"]
    print(f"True adverbs from 48 Laws (first 20): {true_adverbs[:20]}")
except ImportError:
    print("SpaCy not installed. Install with 'pip install spacy' and run 'python -m spacy download en_core_web_sm'.")
except Exception as e:
    print(f"SpaCy error: {e}. Skipping POS tagging.")
